const fs = require('fs');
const path = require('path');
const axios = require('axios');
const FormData = require('form-data');

const SERVER = 'http://localhost:57005';
const FILE = 'input.pdf';

var userContext = "";

async function uploadFile(filePath) {
  const form = new FormData();
  form.append('file', fs.createReadStream(filePath));

  try {
    const res = await axios.post(`${SERVER}/upload`, form, {
      headers: form.getHeaders(),
    });
    console.log('âœ… Upload complete');
    return res.data.filename;
  } catch (err) {
    console.error('âŒ Upload failed:', err.response?.data || err.message);
    process.exit(1);
  }
}

async function fetchChapters(filename) {
  try {
    const res = await axios.get(`${SERVER}/chapters/${filename}`);
    console.log('âœ… Fetched chapters');
    return res.data.chapters;
  } catch (err) {
    console.error('âŒ Chapter fetch failed:', err.response?.data || err.message);
    process.exit(1);
  }
}

async function buildContextHeap(chapters, userContext) {
  const contextHeap = [];

  if (userContext) {
    contextHeap.push(userContext);
  }

  for (const chapter of chapters) {
    contextHeap.push(`${chapter.title}\n${chapter.content}`);
  }

  return contextHeap;
}

function setUserContext(input) {
  userContext = input;
}

async function sendToModel(userContext, chapterText) {
  const fullPrompt = `${userContext}\n\n${chapterText}`;

  const res = await axios.post('http://localhost:11434/api/chat', {
    model: "llama3.2",
    messages: [
      { role: "user", content: fullPrompt }
    ],
    stream: false
  });

  return res.data;
}

async function main() {
  const filename = await uploadFile(FILE);
  console.log(filename);
  const chapters = await fetchChapters(filename);

  console.log(`\nðŸ“– Loaded ${chapters.length} chapters`);
  console.log('\n--- Example Output ---');
  console.log(`Chapter 0 Title: ${chapters[0].title}`);
  console.log(`Chapter 0 Content (preview):\n${chapters[0].content.slice(0, 300)}...`);

  // we need heap since max stack size 8MiB, not enough for 24MiB docs + we lose extendability of context windows for user input
  setUserContext("Test User Context!");
  const heap = await buildContextHeap(chapters, userContext);
  console.log('Context heap built.\n---'); 
  heap.forEach((entry, i) => {
    console.log(`Slot ${i}: ${entry.slice(0, 100)}...`);
  });

  // loading all elements of heap into model as context, need to do this for every prompt generated by this design... prolly need to change later
  // gonna be hell on client side if we repeat this complexity for every prompt instance, is there a way to have it as like a state variable or 
  // saved in a separate client side config so that it can boot into the model automatically, that seems like smart design to me. dont know
  // how to do it though.
  for (let i = 1; i < heap.length; i++) {
    const response = await sendToModel(heap[0], heap[i]); // also need to put in prompt since so this is a terrible design long term
    console.log(`ðŸ“£ Model response for slot ${i}:\n${response.message.content.slice(0, 500)}...\n`);
  } 
  // requires model running at port 11424
}

main();
